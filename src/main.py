"""
ä¸»ç¨‹åºå…¥å£æ¨¡å—
è´Ÿè´£åè°ƒæ•´ä¸ªä»£ç†èŠ‚ç‚¹æ”¶é›†å’Œå¤„ç†æµç¨‹
"""

import os
import time
import requests
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from src.database.models import Session, ProxyNode
from src.collectors.github import get_github_repos, fetch_file_content
from src.collectors.platforms import search_all_platforms
from src.utils.parser import parse_content
from src.utils.validator import is_node_alive_with_china_proxy
from src.utils.china_proxy_reader import is_china_proxy_enabled, get_china_proxy_stats
from src.exporters.subscription import export_subscription
from config.settings import PLATFORM_KEYWORDS

def save_nodes(links, source):
    """
    ä¿å­˜è§£æåˆ°çš„ä»£ç†èŠ‚ç‚¹é“¾æ¥åˆ°æ•°æ®åº“
    
    Args:
        links (list): ä»£ç†èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨
        source (str): èŠ‚ç‚¹æ¥æºæ ‡è¯†
        
    Returns:
        None
    """
    # åˆ›å»ºæ•°æ®åº“ä¼šè¯
    session = Session()

    # åˆå§‹åŒ–è®¡æ•°å™¨
    count = 0  # æˆåŠŸä¿å­˜çš„èŠ‚ç‚¹æ•°é‡
    total = 0  # æ€»å…±å¤„ç†çš„é“¾æ¥æ•°é‡
    skipped_existing = 0  # å·²å­˜åœ¨è€Œè·³è¿‡çš„èŠ‚ç‚¹æ•°é‡
    skipped_dead_or_invalid = 0  # æ— æ•ˆæˆ–å¤±æ•ˆè€Œè·³è¿‡çš„èŠ‚ç‚¹æ•°é‡
    # éå†æ‰€æœ‰ä»£ç†é“¾æ¥
    for link in links:
        total += 1
        # ç”Ÿæˆé“¾æ¥çš„å”¯ä¸€å“ˆå¸Œå€¼
        link_hash = hashlib.md5(link.encode("utf-8")).hexdigest()
        # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­
        exists = session.query(ProxyNode).filter_by(unique_hash=link_hash).first()
        if exists:
            skipped_existing += 1
            continue
        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯ç”¨
        if not is_node_alive_with_china_proxy(link):
            skipped_dead_or_invalid += 1
            continue
        # æå–åè®®ç±»å‹
        protocol = link.split("://")[0]
        # åˆ›å»ºæ–°çš„ä»£ç†èŠ‚ç‚¹å¯¹è±¡
        new_node = ProxyNode(protocol=protocol, link=link, unique_hash=link_hash, source=source)
        # æ·»åŠ åˆ°ä¼šè¯ä¸­
        session.add(new_node)
        count += 1
    try:
        # æäº¤äº‹åŠ¡
        session.commit()
        # æ‰“å°å¤„ç†ç»“æœç»Ÿè®¡ä¿¡æ¯
        print(
            f"[{source}] Parsed {total} links, saved {count} new nodes, "
            f"skipped existing={skipped_existing}, dead/invalid={skipped_dead_or_invalid}."
        )
    except Exception as e:
        # å‘ç”Ÿé”™è¯¯æ—¶å›æ»šäº‹åŠ¡
        session.rollback()
        print(f"Database error: {e}")
    finally:
        # ç¡®ä¿ä¼šè¯è¢«å…³é—­
        session.close()

def fetch_url_content(url):
    """
    è·å–æŒ‡å®šURLçš„å†…å®¹
    
    è¯¥å‡½æ•°é€šè¿‡HTTP GETè¯·æ±‚è·å–æŒ‡å®šURLçš„é¡µé¢å†…å®¹ï¼Œå¹¶å¤„ç†å¯èƒ½å‡ºç°çš„å¼‚å¸¸æƒ…å†µã€‚
    Args:
        url (str): è¦è·å–å†…å®¹çš„URLåœ°å€
        
    Returns:
        str or None: æˆåŠŸæ—¶è¿”å›é¡µé¢å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
    å¼‚å¸¸å¤„ç†:
        æ•è·æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ï¼Œåœ¨å‡ºç°ä»»ä½•é”™è¯¯æ—¶é™é»˜è¿”å›None
    è¶…æ—¶è®¾ç½®:
        è¯·æ±‚è¶…æ—¶æ—¶é—´è®¾ç½®ä¸º10ç§’
    """
    try:  # å°è¯•å‘é€HTTP GETè¯·æ±‚
        resp = requests.get(url, timeout=10)  # å‘é€GETè¯·æ±‚ï¼Œè®¾ç½®10ç§’è¶…æ—¶
        if resp.status_code == 200:  # æ£€æŸ¥å“åº”çŠ¶æ€ç æ˜¯å¦ä¸º200
            return resp.text  # è¿”å›é¡µé¢æ–‡æœ¬å†…å®¹
    except:  # æ•è·æ‰€æœ‰å¼‚å¸¸
        pass
    return None  # é»˜è®¤è¿”å›None

def main():

    """
    ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªæ•°æ®æ”¶é›†å’Œå¤„ç†æµç¨‹
    åŒ…æ‹¬è·å–GitHubä»“åº“ã€å¤„ç†æ–‡ä»¶å†…å®¹ã€æœç´¢ç½‘ç»œæ˜ å°„å¹³å°ä»¥åŠå¯¼å‡ºè®¢é˜…åˆ—è¡¨
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œå†³å®šæ˜¯å¦è¿è¡Œç‰¹å®šåŠŸèƒ½æ¨¡å—
    # RUN_GITHUBæ§åˆ¶æ˜¯å¦å¤„ç†GitHubä»“åº“ï¼ŒRUN_PLATFORMSæ§åˆ¶æ˜¯å¦å¤„ç†ç½‘ç»œæ˜ å°„å¹³å°
    run_github = os.environ.get("RUN_GITHUB", "1") == "1"
    run_platforms = os.environ.get("RUN_PLATFORMS", "1") == "1"

    total_start = time.perf_counter()  # è®°å½•æ€»å¼€å§‹æ—¶é—´

    print("Start collecting...")  # å¼€å§‹æ”¶é›†çš„æç¤ºä¿¡æ¯

    github_start = time.perf_counter()  # è®°å½•GitHubå¤„ç†é˜¶æ®µå¼€å§‹æ—¶é—´
    # GitHubä»“åº“å¤„ç†æµç¨‹
    if run_github:
        repos = get_github_repos()  # è·å–GitHubä»“åº“åˆ—è¡¨
        print(f"Found {len(repos)} repositories.")  # æ‰“å°æ‰¾åˆ°çš„ä»“åº“æ•°é‡

        max_workers = int(os.environ.get("MAX_WORKERS", "8")) or 1  # è®¾ç½®çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°

        def handle_repo(repo):
            """å¤„ç†å•ä¸ªä»“åº“çš„å‡½æ•°"""
            print(f"Processing {repo}...")
            contents = fetch_file_content(repo)  # è·å–ä»“åº“æ–‡ä»¶å†…å®¹
            repo_links = []
            for content in contents:
                links = parse_content(content)  # è§£ææ–‡ä»¶å†…å®¹ä¸­çš„é“¾æ¥
                if links:
                    repo_links.extend(links)  # æ”¶é›†æ‰€æœ‰é“¾æ¥
            return repo, repo_links

        github_results = []  # å­˜å‚¨GitHubå¤„ç†ç»“æœ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:  # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
            futures = [executor.submit(handle_repo, repo) for repo in repos]  # æäº¤æ‰€æœ‰ä»“åº“å¤„ç†ä»»åŠ¡
            for future in as_completed(futures):  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                repo, repo_links = future.result()
                if repo_links:
                    github_results.append((repo, repo_links))  # ä¿å­˜æœ‰é“¾æ¥çš„ç»“æœ

        for repo, repo_links in github_results:
            save_nodes(repo_links, repo)  # ä¿å­˜é“¾æ¥å’Œå¯¹åº”çš„ä»“åº“ä¿¡æ¯
    else:
        print("Skipping GitHub repository collection.")  # è·³è¿‡GitHubä»“åº“æ”¶é›†çš„æç¤ºä¿¡æ¯
    github_end = time.perf_counter()  # è®°å½•GitHubå¤„ç†é˜¶æ®µç»“æŸæ—¶é—´
    print(f"GitHub phase took {github_end - github_start:.2f} seconds.")  # æ‰“å°GitHubå¤„ç†é˜¶æ®µè€—æ—¶

    platform_start = time.perf_counter()  # è®°å½•å¹³å°æœç´¢é˜¶æ®µå¼€å§‹æ—¶é—´
    # ç½‘ç»œæ˜ å°„å¹³å°æœç´¢æµç¨‹
    if run_platforms:
        print("\nSearching network mapping platforms...")  # æœç´¢ç½‘ç»œæ˜ å°„å¹³å°çš„æç¤ºä¿¡æ¯
        urls = search_all_platforms(PLATFORM_KEYWORDS)  # åœ¨æ‰€æœ‰å¹³å°ä¸Šæœç´¢URL
        print(f"Found {len(urls)} URLs from platforms.")  # æ‰“å°ä»å¹³å°æ‰¾åˆ°çš„URLæ•°é‡

        max_workers = int(os.environ.get("MAX_WORKERS", "8")) or 1  # è®¾ç½®çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°

        def handle_url(url):
            """å¤„ç†å•ä¸ªURLçš„å‡½æ•°"""
            print(f"Fetching {url}...")
            content = fetch_url_content(url)  # è·å–URLå†…å®¹
            if not content:
                return url, []
            links = parse_content(content)  # è§£æå†…å®¹ä¸­çš„é“¾æ¥
            if not links:
                return url, []
            return url, links

        platform_results = []  # å­˜å‚¨å¹³å°æœç´¢ç»“æœ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:  # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
            futures = [executor.submit(handle_url, url) for url in urls]  # æäº¤æ‰€æœ‰URLå¤„ç†ä»»åŠ¡
            for future in as_completed(futures):  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                url, links = future.result()
                if links:
                    platform_results.append((url, links))  # ä¿å­˜æœ‰é“¾æ¥çš„ç»“æœ

        for url, links in platform_results:
            save_nodes(links, f"platform:{url}")  # ä¿å­˜é“¾æ¥å’Œå¯¹åº”çš„å¹³å°URLä¿¡æ¯
    else:
        print("Skipping network mapping platforms.")  # è·³è¿‡ç½‘ç»œæ˜ å°„å¹³å°æœç´¢çš„æç¤ºä¿¡æ¯
    platform_end = time.perf_counter()  # è®°å½•å¹³å°æœç´¢é˜¶æ®µç»“æŸæ—¶é—´
    print(f"Platform phase took {platform_end - platform_start:.2f} seconds.")  # æ‰“å°å¹³å°æœç´¢é˜¶æ®µè€—æ—¶

    # æ˜¾ç¤ºä¸­å›½ä»£ç†çŠ¶æ€
    if is_china_proxy_enabled():
        proxy_stats = get_china_proxy_stats()
        print(f"\nğŸ‡¨ğŸ‡³ ä¸­å›½ä»£ç†çŠ¶æ€: å¯ç”¨ ({proxy_stats.get('working_count', 0)} ä¸ªä»£ç†)")
        print(f"ä»£ç†å°†ç”¨äºèŠ‚ç‚¹å¯ç”¨æ€§æµ‹è¯•ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§")
    else:
        print(f"\nğŸ‡¨ğŸ‡³ ä¸­å›½ä»£ç†çŠ¶æ€: æœªå¯ç”¨æˆ–æ— å¯ç”¨ä»£ç†")
        print(f"å°†ä½¿ç”¨ç›´æ¥è¿æ¥è¿›è¡ŒèŠ‚ç‚¹å¯ç”¨æ€§æµ‹è¯•")

    export_start = time.perf_counter()  # è®°å½•å¯¼å‡ºé˜¶æ®µå¼€å§‹æ—¶é—´
    export_subscription()  # å¯¼å‡ºæœ€ç»ˆçš„è®¢é˜…åˆ—è¡¨
    export_end = time.perf_counter()  # è®°å½•å¯¼å‡ºé˜¶æ®µç»“æŸæ—¶é—´
    print(f"Export phase took {export_end - export_start:.2f} seconds.")  # æ‰“å°å¯¼å‡ºé˜¶æ®µè€—æ—¶

    total_end = time.perf_counter()  # è®°å½•æ€»ç»“æŸæ—¶é—´
    print(f"Total elapsed time: {total_end - total_start:.2f} seconds.")  # æ‰“å°æ€»è€—æ—¶
    print("Done.")  # å®Œæˆå¤„ç†çš„æç¤ºä¿¡æ¯

if __name__ == "__main__":
    main()